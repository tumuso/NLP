{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_chapter1_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NLP 처리과정 \n",
        "토큰화 -> 정체,추출 -> 인코딩"
      ],
      "metadata": {
        "id": "Jlj-VissxOIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.표준 토큰화\n",
        "자연어 처리에 사용되는 대표적인 파이썬 패키지는 NLTK\n",
        "\n",
        "1.1 표준 토큰화\n",
        "treebank 사용 \n"
      ],
      "metadata": {
        "id": "OmLHR4u1e46X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYH4V3ADfaU7",
        "outputId": "1744c8c9-c75e-4d21-fb2d-928e491d0ce1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DRV44Iyey01",
        "outputId": "c3bfd109-e3e1-48f2-aadd-5b1c48f9c69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer  = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 토큰화 라이브러리\n",
        "여러종류의 tokenizer 가 있음 word_tokenizer"
      ],
      "metadata": {
        "id": "Uabjn6EXf937"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnbwCnolgfjz",
        "outputId": "bb8260b7-b493-4066-cc96-da3460d11575"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJbwfLILe3hO",
        "outputId": "9c906316-b5ec-42ab-c852-789de1548045"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 어간 추출 및 표제어 추출\n",
        "단어의 형태소 level에서 분석을 하게 되면 다른 품사 또는 다른 시제의 단어라고 해도 같은 형태로 토큰화 가능\n",
        "\n",
        "-->둘의 차이 : 품사의 태깅 유무\n",
        "\n",
        "-->표제어 : v, n 태깅 가능\n",
        "\n",
        "-->어간 : 태깅 불가능\n",
        "\n",
        "2.1 어간 추출Stemmer vs 표제어 추출lemmazation \n",
        "\n",
        "대표적 어간 추출 기법은 porter 추출 패키지"
      ],
      "metadata": {
        "id": "t9T_Zgh2ufHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#어간 추출\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "stem1 = PorterStemmer()\n",
        "stem2 = LancasterStemmer()\n",
        "words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n",
        "print(\"Poter Stemmer :\", [stem1.stem(w) for w in words])\n",
        "print(\"Lancaster Stemmer :\", [stem2.stem(w) for w in words]) #왜 ate를 at으로..?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DIn4jVVe3jb",
        "outputId": "f0b2743d-c0a4-4ba4-a85e-9bb5690d15d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poter Stemmer : ['eat', 'ate', 'eaten', 'eat']\n",
            "Lancaster Stemmer : ['eat', 'at', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#표제어 추출\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemm = WordNetLemmatizer()\n",
        "words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n",
        "print('WordNet Lemmetizer  :', [lemm.lemmatize(w, pos='v') for w in words]) #pos로 단어의 품사 지정해줌\n",
        "#모든 시제를 동사원형으로 바꿔줌 품사태깅이 가능하다면 표제어 추출이 좋음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJu7H2qtupwD",
        "outputId": "decc3fc7-024b-472b-eea0-357a65bff764"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "WordNet Lemmetizer  : ['eat', 'eat', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 불용어 제거\n",
        "3.1불용어 예시\n",
        "\n",
        "영어의 불용어 예시 \n",
        "\n",
        "stopword단어 데이터를 받기 위한 사전작업이 필요함"
      ],
      "metadata": {
        "id": "BtaBjsary9bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olQoFKcge3lr",
        "outputId": "d3f87729-07dd-4d3d-f4bd-3c22690b3af0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "input_sentence = \"we should all study hard for the exam.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#토큰화 후 불용어제거 for문\n",
        "\n",
        "word_tokens = word_tokenize(input_sentence)\n",
        "result = []\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    result.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1au86-Xre3nt",
        "outputId": "47106214-0e9e-4051-ebbe-d67da3cc8eba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['we', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam', '.']\n",
            "['study', 'hard', 'exam', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 정수 인코딩 및 sorting\n",
        "4.1 Enumerate 사용 "
      ],
      "metadata": {
        "id": "nO-shqNVf64J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mylist = ['Enflish', 'math','Science' ]\n",
        "for n, name in enumerate(mylist):\n",
        "  print('Course : {}, Number : {}'.format(name,n))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "2-Ck6BP-e3p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd00577-258b-485c-95a9-ce65984cdcc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Course : Enflish, Number : 0\n",
            "Course : math, Number : 1\n",
            "Course : Science, Number : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 정수 인코딩 및 High-frequency Sorting"
      ],
      "metadata": {
        "id": "05POlretgaTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'apple':2,'July':6,'piano':4,'cup':8,'orange':1} #BoW\n",
        "vocab_sort = sorted(vocab.items(), key= lambda x:x[1], reverse = True) #슛자 기준 역순 정렬\n",
        "print(vocab_sort)\n",
        "\n",
        "# 많이 사용된 벡터에 1을 부여하고 싶음 -> word[0] = cup, index enumerate하면서 1씩 증가\n",
        "word2inx = {word[0] : index +1 for index, word in enumerate(vocab_sort)}\n",
        "print(word2inx)"
      ],
      "metadata": {
        "id": "5ul5fgCce3sF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b92bda6-6857-4413-c4ba-6bcef29a5d06"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n",
            "{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bow로 만들어진 토큰화의 결과(Bow)를 가장 높은 빈도수부터 재정렬하고 이를 통해 정수 인코딩 진행"
      ],
      "metadata": {
        "id": "fI_39UDRiKgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy.\"\\\n",
        "       \"but some of Model-based RL algorithms do have a value function.\"\n",
        "token_text = tokenizer.tokenize(text)\n",
        "word2inx = {}\n",
        "Bow = []\n",
        "for word in token_text:\n",
        "  if word not in word2inx.keys():\n",
        "    word2inx[word] = len(word2inx) #비어있는 상태의 딕셔너리 길이 즉 0부터 시작 ex> Model = 0 \n",
        "    Bow.insert(len(word2inx)-1,1) #(0,1)에 insert\n",
        "  else:\n",
        "    inx= word2inx.get(word) #get함수의 의미 : 기존에 있는 word를 가져오라는 의미\n",
        "    Bow[inx] +=1 #가져온 word값에 +1\n",
        "\n",
        "print(word2inx) # inx\n",
        "print(Bow) #빈도수를 고려해서 인코딩\n"
      ],
      "metadata": {
        "id": "A4YDZNl9e3uS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df041f11-3a4f-4d58-ca79-734f361f2b4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policy.but': 10, 'some': 11, 'of': 12, 'algorithms': 13, 'have': 14, '.': 15}\n",
            "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장으로부터  토큰화를 통해 토큰 리스트를 만들고, 이를 이용해서 BoW를 생성하는 전체 알고리즘 word2inx = {}를 만들고, 리스트에 없는 단어의 경우 새로 리스트와 BoW에 단어를 추가하고 리스트에 있는 단어는 inx +=1"
      ],
      "metadata": {
        "id": "sCcPywEsle_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 유사도 분석\n",
        "\n",
        "5.1코사인 유사도"
      ],
      "metadata": {
        "id": "bIpFM4D-lq8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# 두 벡터 사이의 각도의 코사인값이 코사인 유사도 cf>cos0=1, cos90 = 0, cos180 = -1\n",
        "# 내적 = a,b의 norm값 x cos0      *이때 내적은 각 성분끼리의 곱의 합산\n",
        "def cos_sim(A,B):\n",
        "  return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B))\n",
        "\n",
        "a = [1,0,0,1]\n",
        "b = [0,1,1,0]\n",
        "c = [1,1,1,1]\n",
        "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
      ],
      "metadata": {
        "id": "IPWDid94e3wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3941b184-44ee-4ab7-fdb2-92457a155616"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 0.7071067811865475 0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2레반슈타인 거리"
      ],
      "metadata": {
        "id": "ckKEt3vUpIsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#어떻게하면 최소한의 수정단위를 거쳐서 처음단어에서 나중단어로 갈것인지 보여주는 거리\n",
        "#추가, 삭제, 수정중 거리가 가장 짧은것을 return\n",
        "\n",
        "def leven(text1, text2):\n",
        "  len1 = len(text1) +1\n",
        "  len2 = len(text2) +1\n",
        "\n",
        "  sim_array = np.zeros((len1, len2)) #matrix테이블 만들기\n",
        "  sim_array[:,0] = np.linspace(0, len1-1, len1) #모든행 첫열에 0~len1-1길이만큼 len1만큼 등분해서 숫자를 채워달라\n",
        "  sim_array[0,:] = np.linspace(0, len2-1, len2) #0번째 행 모든 열에 마찬가지로 linspace만큼 채워달라\n",
        "  for i in range(1, len1):\n",
        "    for j in range(1, len2):\n",
        "      add_char = sim_array[i-1,j] +1 # 추가는 위(row)의 값에 +1\n",
        "      sub_char = sim_array[i,j-1] +1 # 삭제는 옆(column의 값에 +1 \n",
        "      if text1[i-1] == text2[j-1]:\n",
        "        mod_char = sim_array[i-1, j-1] #수정은 대각선에 있는것이 같으면 그대로, 다르면 +1 대각선 위치(i-1, j-1)\n",
        "      else:\n",
        "        mod_char = sim_array[i-1, j-1] + 1\n",
        "      sim_array[i,j] = min([add_char, sub_char, mod_char]) # 추가, 삭제, 수정중 가장 짧은것을 return\n",
        "  return sim_array[-1,-1]  # (-1,-1)의 위치 = array의 오른쪽 끝\n",
        "\n",
        "print(leven('데이터마이닝', '데이타마닝'))\n",
        "\n"
      ],
      "metadata": {
        "id": "tWex7Vg-e3yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56923dfe-545d-4d9c-8f26-ff548d374946"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Word2Vec - CBoW, SkipGrim\n",
        "\n",
        "6.1 CBow와 SkipGram을 위한 전처리 복습 및 Overview"
      ],
      "metadata": {
        "id": "WEZ_Ywp95a3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "data = pd.read_csv('transcripts.csv')\n",
        "print('Missing Values :' , data.isnull().sum())\n",
        "data = data.dropna().reset_index(drop=True)\n",
        "\n",
        "#한줄씩 나뉘어진 data를 str로 받아서 합치는 작업\n",
        "\n",
        "merge_data = ''.join(str(data.iloc[i,0]) for i in range(100))\n",
        "print('Total word count: ', len(merge_data))"
      ],
      "metadata": {
        "id": "9YFvSk6ge303"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3:13\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "token_text = tokenizer.tokenize(merge_data)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "token_stop_text = []\n",
        "for w in token_text:\n",
        "  if w not in stop_words:\n",
        "    token_stop_text.append(w)\n",
        "\n",
        "print('After cleaning: ', len(token_stop_text))"
      ],
      "metadata": {
        "id": "4f285OOXe325"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5DS3r62je35B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}