{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_chapter1_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NLP 처리과정 \n",
        "토큰화 -> 정체,추출 -> 인코딩"
      ],
      "metadata": {
        "id": "Jlj-VissxOIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.표준 토큰화\n",
        "자연어 처리에 사용되는 대표적인 파이썬 패키지는 NLTK\n",
        "\n",
        "1.1 표준 토큰화\n",
        "treebank 사용 \n"
      ],
      "metadata": {
        "id": "OmLHR4u1e46X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYH4V3ADfaU7",
        "outputId": "1744c8c9-c75e-4d21-fb2d-928e491d0ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DRV44Iyey01",
        "outputId": "c3bfd109-e3e1-48f2-aadd-5b1c48f9c69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer  = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 토큰화 라이브러리\n",
        "여러종류의 tokenizer 가 있음 word_tokenizer"
      ],
      "metadata": {
        "id": "Uabjn6EXf937"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnbwCnolgfjz",
        "outputId": "bb8260b7-b493-4066-cc96-da3460d11575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJbwfLILe3hO",
        "outputId": "9c906316-b5ec-42ab-c852-789de1548045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 어간 추출 및 표제어 추출\n",
        "단어의 형태소 level에서 분석을 하게 되면 다른 품사 또는 다른 시제의 단어라고 해도 같은 형태로 토큰화 가능\n",
        "\n",
        "-->둘의 차이 : 품사의 태깅 유무\n",
        "\n",
        "-->표제어 : v, n 태깅 가능\n",
        "\n",
        "-->어간 : 태깅 불가능\n",
        "\n",
        "2.1 어간 추출Stemmer vs 표제어 추출lemmazation \n",
        "\n",
        "대표적 어간 추출 기법은 porter 추출 패키지"
      ],
      "metadata": {
        "id": "t9T_Zgh2ufHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#어간 추출\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "stem1 = PorterStemmer()\n",
        "stem2 = LancasterStemmer()\n",
        "words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n",
        "print(\"Poter Stemmer :\", [stem1.stem(w) for w in words])\n",
        "print(\"Lancaster Stemmer :\", [stem2.stem(w) for w in words]) #왜 ate를 at으로..?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DIn4jVVe3jb",
        "outputId": "f0b2743d-c0a4-4ba4-a85e-9bb5690d15d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poter Stemmer : ['eat', 'ate', 'eaten', 'eat']\n",
            "Lancaster Stemmer : ['eat', 'at', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#표제어 추출\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemm = WordNetLemmatizer()\n",
        "words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n",
        "print('WordNet Lemmetizer  :', [lemm.lemmatize(w, pos='v') for w in words]) #pos로 단어의 품사 지정해줌\n",
        "#모든 시제를 동사원형으로 바꿔줌 품사태깅이 가능하다면 표제어 추출이 좋음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJu7H2qtupwD",
        "outputId": "decc3fc7-024b-472b-eea0-357a65bff764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "WordNet Lemmetizer  : ['eat', 'eat', 'eat', 'eat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 불용어 제거\n",
        "3.1불용어 예시\n",
        "\n",
        "영어의 불용어 예시 \n",
        "\n",
        "stopword단어 데이터를 받기 위한 사전작업이 필요함"
      ],
      "metadata": {
        "id": "BtaBjsary9bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english')[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olQoFKcge3lr",
        "outputId": "d3f87729-07dd-4d3d-f4bd-3c22690b3af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "input_sentence = \"we should all study hard for the exam.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#토큰화 후 불용어제거 for문\n",
        "\n",
        "word_tokens = word_tokenize(input_sentence)\n",
        "result = []\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    result.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1au86-Xre3nt",
        "outputId": "47106214-0e9e-4051-ebbe-d67da3cc8eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['we', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam', '.']\n",
            "['study', 'hard', 'exam', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 정수 인코딩 및 sorting\n",
        "4.1 Enumerate 사용 "
      ],
      "metadata": {
        "id": "nO-shqNVf64J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mylist = ['Enflish', 'math','Science' ]\n",
        "for n, name in enumerate(mylist):\n",
        "  print('Course : {}, Number : {}'.format(name,n))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "2-Ck6BP-e3p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd00577-258b-485c-95a9-ce65984cdcc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Course : Enflish, Number : 0\n",
            "Course : math, Number : 1\n",
            "Course : Science, Number : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 정수 인코딩 및 High-frequency Sorting"
      ],
      "metadata": {
        "id": "05POlretgaTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'apple':2,'July':6,'piano':4,'cup':8,'orange':1} #BoW\n",
        "vocab_sort = sorted(vocab.items(), key= lambda x:x[1], reverse = True) #슛자 기준 역순 정렬\n",
        "print(vocab_sort)\n",
        "\n",
        "# 많이 사용된 벡터에 1을 부여하고 싶음 -> word[0] = cup, index enumerate하면서 1씩 증가\n",
        "word2inx = {word[0] : index +1 for index, word in enumerate(vocab_sort)}\n",
        "print(word2inx)"
      ],
      "metadata": {
        "id": "5ul5fgCce3sF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b92bda6-6857-4413-c4ba-6bcef29a5d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n",
            "{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bow로 만들어진 토큰화의 결과(Bow)를 가장 높은 빈도수부터 재정렬하고 이를 통해 정수 인코딩 진행"
      ],
      "metadata": {
        "id": "fI_39UDRiKgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Model-based RL don't need a value function for the policy.\"\\\n",
        "       \"but some of Model-based RL algorithms do have a value function.\"\n",
        "token_text = tokenizer.tokenize(text)\n",
        "word2inx = {}\n",
        "Bow = []\n",
        "for word in token_text:\n",
        "  if word not in word2inx.keys():\n",
        "    word2inx[word] = len(word2inx) #비어있는 상태의 딕셔너리 길이 즉 0부터 시작 ex> Model = 0 \n",
        "    Bow.insert(len(word2inx)-1,1) #(0,1)에 insert\n",
        "  else:\n",
        "    inx= word2inx.get(word) #get함수의 의미 : 기존에 있는 word를 가져오라는 의미\n",
        "    Bow[inx] +=1 #가져온 word값에 +1\n",
        "\n",
        "print(word2inx) # inx\n",
        "print(Bow) #빈도수를 고려해서 인코딩\n"
      ],
      "metadata": {
        "id": "A4YDZNl9e3uS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df041f11-3a4f-4d58-ca79-734f361f2b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policy.but': 10, 'some': 11, 'of': 12, 'algorithms': 13, 'have': 14, '.': 15}\n",
            "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장으로부터  토큰화를 통해 토큰 리스트를 만들고, 이를 이용해서 BoW를 생성하는 전체 알고리즘 word2inx = {}를 만들고, 리스트에 없는 단어의 경우 새로 리스트와 BoW에 단어를 추가하고 리스트에 있는 단어는 inx +=1"
      ],
      "metadata": {
        "id": "sCcPywEsle_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 유사도 분석\n",
        "\n",
        "5.1코사인 유사도"
      ],
      "metadata": {
        "id": "bIpFM4D-lq8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# 두 벡터 사이의 각도의 코사인값이 코사인 유사도 cf>cos0=1, cos90 = 0, cos180 = -1\n",
        "# 내적 = a,b의 norm값 x cos0      *이때 내적은 각 성분끼리의 곱의 합산\n",
        "def cos_sim(A,B):\n",
        "  return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B))\n",
        "\n",
        "a = [1,0,0,1]\n",
        "b = [0,1,1,0]\n",
        "c = [1,1,1,1]\n",
        "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
      ],
      "metadata": {
        "id": "IPWDid94e3wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3bff3df-4576-41b7-dade-f85e714c61c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 0.7071067811865475 0.7071067811865475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2레반슈타인 거리"
      ],
      "metadata": {
        "id": "ckKEt3vUpIsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#어떻게하면 최소한의 수정단위를 거쳐서 처음단어에서 나중단어로 갈것인지 보여주는 거리\n",
        "#추가, 삭제, 수정중 거리가 가장 짧은것을 return 추가, 삭제는 +1, 대각선위치면 그대로\n",
        "\n",
        "def leven(text1, text2):\n",
        "  len1 = len(text1) +1\n",
        "  len2 = len(text2) +1\n",
        "\n",
        "  sim_array = np.zeros((len1, len2)) #matrix테이블 만들기\n",
        "  sim_array[:,0] = np.linspace(0, len1-1, len1) #모든행 첫열에 0~len1-1길이만큼 len1만큼 등분해서 숫자를 채워달라\n",
        "  sim_array[0,:] = np.linspace(0, len2-1, len2) #0번째 행 모든 열에 마찬가지로 linspace만큼 채워달라\n",
        "  for i in range(1, len1):\n",
        "    for j in range(1, len2):\n",
        "      add_char = sim_array[i-1,j] +1 # 추가는 위(row)의 값에 +1\n",
        "      sub_char = sim_array[i,j-1] +1 # 삭제는 옆(column의 값에 +1 \n",
        "      if text1[i-1] == text2[j-1]:\n",
        "        mod_char = sim_array[i-1, j-1] #수정은 대각선에 있는것이 같으면 그대로, 다르면 +1 대각선 위치(i-1, j-1)\n",
        "      else:\n",
        "        mod_char = sim_array[i-1, j-1] + 1\n",
        "      sim_array[i,j] = min([add_char, sub_char, mod_char]) # 추가, 삭제, 수정중 가장 짧은것을 return\n",
        "  return sim_array[-1,-1]  # (-1,-1)의 위치 = array의 오른쪽 끝\n",
        "\n",
        "print(leven('데이터마이닝', '데이타마닝'))\n",
        "\n"
      ],
      "metadata": {
        "id": "tWex7Vg-e3yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421631d1-6d1b-4867-e9ce-d9bdb20f2bac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Word2Vec - CBoW, SkipGrim\n",
        "\n",
        "6.1 CBow와 SkipGram을 위한 전처리 복습 및 Overview"
      ],
      "metadata": {
        "id": "WEZ_Ywp95a3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "data = pd.read_csv('transcripts.csv')\n",
        "print('Missing Values :' , data.isnull().sum())\n",
        "data = data.dropna().reset_index(drop=True) \n",
        "\n",
        "#한줄씩 나눠진 data를 str로 받아서 합치는 작업 df에 iloc for문으로 할당 \n",
        "\n",
        "merge_data = ''.join(str(data.iloc[i,0]) for i in range(100))\n",
        "print('Total word count: ', len(merge_data))\n",
        "print(merge_data[:40])"
      ],
      "metadata": {
        "id": "9YFvSk6ge303"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "token_text = tokenizer.tokenize(merge_data)\n",
        "\n",
        "#불용어제거\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "token_stop_text = []\n",
        "for w in token_text:\n",
        "  if w not in stop_words:\n",
        "    token_stop_text.append(w)\n",
        "\n",
        "print('After cleaning: ', len(token_stop_text))"
      ],
      "metadata": {
        "id": "4f285OOXe325"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2inx = {}\n",
        "Bow = []\n",
        "for word in token_stop_text:\n",
        "  if word not in word2inx.keys():\n",
        "    word2inx[word] = len(word2inx)\n",
        "    Bow.insert(len(word2inx)-1,1)  #원하는 위치에 추가할때 insert 사용 \n",
        "  else:\n",
        "    inx = word2inx.get(word)\n",
        "    Bow[inx] += 1\n",
        "\n",
        "print('Unique Words Count :' , len(Bow))"
      ],
      "metadata": {
        "id": "5DS3r62je35B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2 nltk 내장 함수를 이용한 CBow 학습"
      ],
      "metadata": {
        "id": "Q0nFYI16eGMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "token_stop_text = np.reshape(np.array(token_stop_text),[-1,1])\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#vector size: 임베딩벡터의 사이즈\n",
        "#sg : skip-gram은 1, CBow는 0\n",
        "#min_count 최소 빈도 미만이면 제거\n",
        "#window 참조할 주변 벡터 크기 \n",
        "\n",
        "model = Word2Vec(vector_size = 100, window = 5 , min_count = 2 ,sg = 0)\n",
        "model.build_vocab(token_stop_text)\n",
        "model.train(token_stop_text, total_examples = model.corpus_count , epochs = 30, report_delay = 1)\n",
        "vocabs = model.wv.key_to_index.keys() #단어의 임베딩 형태 확인\n",
        "word_vec_list = [model.wv[i] for i in vocabs] #각 vocab의 imbedded vector확인\n",
        "           "
      ],
      "metadata": {
        "id": "DnIcGgKQeFco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 PCA를 통한 학습 모델 시각화\n",
        "\n",
        "-16600개의 단어 각각 100개의 벡터가 임베딩 되어있음\n",
        "-시각화를 위해 100차원을 ->2차원으로 축소"
      ],
      "metadata": {
        "id": "pKPbrhQg_kAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components= 2)\n",
        "pcafit = pca.fit_transform(word_vec_list)\n",
        "x = pcafit[0:50,0]\n",
        "y = pcafit[0:50, 1]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x, y, marker = 'o')\n",
        "for i , v in enumerate(vocabs):\n",
        "  if i <=49:\n",
        "    plt.annotate(v , xy = (x[i], y[i]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6vckv1I0eFhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. SGNS : SkipGram with Negative Sampling\n",
        "\n",
        "7.1 SkipGram 전용 Dataset 구성\n",
        "\n",
        "토큰화된 결과만 필요했던 CBOW 및 SkipGram과는 달리 SGNS는 두 단어의 인접 여부가 labeling에서 제공하는 전처리 도구를 활용\n",
        "기본적인 토큰화 과정을 거친 후에 skipgram 함수를 이용하여 변환\n",
        "단어 사이가 근접하면 1, 근접하지 않으면0\n",
        "비슷한 애들끼리 임베딩 벡터의 차이를 작게하기 위함"
      ],
      "metadata": {
        "id": "AoKOjSbSAq6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('trancripts.csv')\n",
        "print('Missing Values :', data.isnull().sum())\n",
        "data = data.dropna().reset_index(drop=True)\n",
        "merge_data = ''.join(str(data.iloc[i,0]) for i in range(30))\n",
        "print('Total word count: ', len(merge_data))\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "token_text = tokenizer.tokenize(merge_data)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "token_stop_text = []\n",
        "for w in token_text:\n",
        "  if w not in stop_words:\n",
        "    token_stop_text.append(w)\n",
        "\n",
        "print('After cleaning :' , len(token_stop_text)"
      ],
      "metadata": {
        "id": "x_5i3I9aAqbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.kears.preprocessing.text import tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(token_stop_text)\n",
        "word2inx = tokenizer.word_index\n",
        "encoded = tokenizer.texts_to_sequences(token_stop_text)\n",
        "encoded = np.array(encoded).T\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "skip_gram = [skipgrams(sample, vocabulary_size = len(word2inx)+1,\n",
        "                       window_size= 10) for sample in encoded]\n",
        "#dataset의 구성 : text1, text2, ... label\n",
        "                     "
      ],
      "metadata": {
        "id": "-1t_7wtDAqhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import FTPHandler\n",
        "from traitlets.traitlets import Float\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import LongTensor as LT\n",
        "from torch import FloatTensor as FT"
      ],
      "metadata": {
        "id": "MMTAF9inAql1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec의 원리는 각 단어마다 지정된 고유 벡터 값을 생성하는 것"
      ],
      "metadata": {
        "id": "lQn8XH1XIN2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.embedding(vocab, embed) : 정수 인코딩된 결과를 넣으면 word2vec 임베딩 결과를 나타내줌\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_size = embed_size\n",
        "    self.word1_vector = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "    self.word2_vector = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "    self.word1_vector.weight = nn.Parameter(torch.cat([torch.zeros(1, self.embed_size), FT(self.vocab_size-1,\n",
        "                                                                                           self.embed_size).uniform(-0.1, 0.1)]))\n",
        "    self.word2_vector.weight = nn.Parameter(torch.cat([torch.zeros(1, self.embed_size), FT(self.vocab_size-1,\n",
        "                                                                                           self.embed_size).uniform(-0.1, 0.1)]))\n",
        "    self.word1_vector.weight.requires_gard = True\n",
        "    self.word2_vector.weight.requires_gard = True\n",
        "  \n",
        "# pytorch 사용하기 위해서 tensor에 래핑 LT\n",
        "# GPU를 사용한다면 GPU도 래핑해줌 \n",
        "\n",
        "  def forward_word1(self, data):\n",
        "    vec = LT(data)\n",
        "    vec = vec.cuda() if self.word1_vector.weight.is_cuda else vec\n",
        "    return self.word1_vector(vec)\n",
        "\n",
        "  def forward_word2(self, data):\n",
        "    vec = LT(data)\n",
        "    vec = vec.cuda() if self.word2_vector.weight.is_cuda else vec\n",
        "    return self.word2_vector(vec)\n"
      ],
      "metadata": {
        "id": "w8Cs4XS-Hq2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.bmm\n",
        "\n",
        "[B, n, m] x [B, m, p] = [B, n. p]\n",
        "\n",
        "유사도가 비슷하다면 2개의 dot값이 작으면 좋겠음\n",
        "\n",
        "word1 -> embed  1\n",
        "\n",
        "word2 -> embed  2\n",
        "\n",
        "1.2를 내적하고 싶음\n",
        "\n",
        "[B , m]\n",
        "\n",
        "[B , m]  이 둘에 차원을 추가하면 내적가능\n",
        "\n",
        "ex> [1,m]x[mx1] -> [1,1] 스칼라 matrix : 벡터의 내적결과\n",
        "\n",
        "word1 [B,m] -> [B.1.m]\n",
        "\n",
        "word2 [B,m] -> [B,m,1]\n",
        "\n",
        "label [B] -> [B,1]\n",
        "\n",
        "추가된1은 unsqueeze 함수를 통해 찾기 가능"
      ],
      "metadata": {
        "id": "t_oAe-lKyuA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import WEXITED\n",
        "class SGNS(nn.Module):\n",
        "  def __init__(self, embed, vocab_size):\n",
        "    super(SGNS, self).__init__()\n",
        "    slef.embed = embed\n",
        "    self.vocab_size = vocab_size\n",
        "    self.weights = None\n",
        "  \n",
        "  def forward(self, word1, word2, label):\n",
        "    word1 = self.embed.forward_word1(word1).unsqueeze(1)\n",
        "    word2 = self.embed.forward_word2(word2).unsqueeze(2)\n",
        "    label = LT(label).unsqueeze(1)\n",
        "    prediction = torch.bmm(word1, word2).squeeze(2).sigmoid().log()\n",
        "    loss = -label * prediction\n",
        "    return loss,mean()\n",
        "\n",
        "\n",
        "#squeeze를 통해 추가된 차원을 없앨 수 있음 위는 [B.1.1]이었으나 squeeze를 통해 [B,1]\n"
      ],
      "metadata": {
        "id": "WuHZGaql0acK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q) torch.bmm의 계산 형태\n",
        "  \n",
        "*왜 MLP의 형태처럼 loss계산이 불가능한가?\n",
        "\n",
        "*negative sampling을 어떻게 처리하고 있는가?\n",
        "\n",
        "Q) 이 코드에서 loss로 정의된 term이 cross-entropy를 대신할 수 있는 이유는?\n",
        "\n",
        "Q) .unsqueeze(), .squeeze() --> 차원 확장, 차원 축소의 역할"
      ],
      "metadata": {
        "id": "FmXd33QK18rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDdataset\n",
        "from tqdm import tqdm\n",
        "vocab_size = len(word2idx)+1\n",
        "word2vec = Word2Vec(vocab_size = vocab_size, embed_size = 100)\n",
        "sgns = SGNS(embed = word2vec, vocab_size = vocab_size)\n",
        "optim = Adam(sgns.parameters())\n",
        "print('Train Ready')"
      ],
      "metadata": {
        "id": "o-XQbg0JytZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data를 tensor로 래핑한 후 , dataloader로 불러옴\n",
        "\n",
        "for _, element in enumerate(skip_gram):\n",
        "  word1 = LT(np.array(list(zip(*element[0]))[0], dtype = 'int32'))\n",
        "  word2 = LT(np.array(list(zip(*element[0]))[1], dtype = 'int32'))\n",
        "  label = LT(np.array(element[1], dtype = 'int32'))\n",
        "  dataset = TensorDataset(word1, word2, label)\n",
        "  train_loader = DataLoader(dataset, batch_size = 256, shuffle = True) #미니배치 실행될때마다 shuffle\n",
        "\n",
        "print('Data Loaded')  "
      ],
      "metadata": {
        "id": "w4xubvXl1z6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  with tqdm(train_loader, unit = 'batch') as tepoch:\n",
        "    for word1, word2, label in tepoch:\n",
        "      loss = sgns(word1, word2, label)\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      tepoch.set_descroption(f\"Epoch {epoch}\")\n",
        "      tepoch.set_postfix(loss = loss.item)\n",
        "      \n",
        "# label이 1인 애들의 내적이 줄어드는 방향으로 학습완료"
      ],
      "metadata": {
        "id": "TXVHg01o1z87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 검증 위해 word2vec에 imbedding vector를 가져와서 확인\n",
        "f = open('vectors.txt', 'w')\n",
        "ww = 0\n",
        "f.write('{} {}\\n'.format(7930, 100))\n",
        "vectors = word2vec.word1_vector.weight.detach().numpy()\n",
        "for i, v in enumerate(word2idx.keys()):\n",
        "  try:\n",
        "    f.write('{} {}\\n'.format(v, ' '.join(map(str, list(vectots[i+1, :])))))\n",
        "    ww += 1\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "f.close()\n",
        "\n",
        "import gensim\n",
        "embed_word2vec = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary = False)\n"
      ],
      "metadata": {
        "id": "rHLINOek1z_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_word2vec.most_similar(postive = ['obey'])"
      ],
      "metadata": {
        "id": "JakAmlei10B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_word2vec.most_similar(postive = ['love'])"
      ],
      "metadata": {
        "id": "OvtIDEFX6FMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OZCOoZP76FKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pb7N37Lr6FIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NPInhDIU6FEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UeXPgihL6E65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yPaLQe5L6EoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}